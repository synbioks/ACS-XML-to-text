{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACS XML Parser\n",
    "This parser is used to extract the text, sequences, and meta data from ACS articles. The parsing process has two parts: First it works on the article xml files to extract the text and meta data from them, then it looks at the supplementary files for genetic sequences. At the end, the parser outputs the extracted data as pickle files for downstream tasks. For a more detailed specification on the input/output formats, please visit the [Github repo](https://github.com/synbioks/ACS-XML-to-text/tree/acs-parser-jiawei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "To run this script, we recommend using anaconda environment:\n",
    "\n",
    "Create a new environment\n",
    "```\n",
    "conda create -n acs-parser python=3.6 \n",
    "conda activate acs-parser\n",
    "```\n",
    "\n",
    "Install the required packages:\n",
    "```\n",
    "conda install lxml tqdm \n",
    "pip install python-docx\n",
    "apt-get -y install poppler-utils\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import requests\n",
    "import pickle\n",
    "import json\n",
    "import subprocess as sp\n",
    "from docx import Document\n",
    "from pprint import pprint\n",
    "from lxml import etree\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data\n",
    "The input to this script is the data dump from ACS. The data dump consist of folders where each folder contains one ACS article in xml format and one folder called suppl. The suppl folder has all the supplementary files related to that article.\n",
    "\n",
    "example:\n",
    "```\n",
    "sb000001           <- this is one folder, and we refer \"sb000001\" as the publication number\n",
    "|-- sb000001.xml   <- this is the article file\n",
    "+-- suppl          <- this folder contains all the supplementary file of this article\n",
    "    |-- file_1.pdf\n",
    "    +-- file_2.pdf\n",
    "```\n",
    "\n",
    "Currently there are 1597 folders like the example in the ACS data dump, and you can find those folders in `/sbksvol/data/acs-data/article-files`. We call this path the input path of the parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Arguments\n",
    "The parser takes three arguments:\\\n",
    "`input_path` tells the parser where to look for ACS data dump.\\\n",
    "`output_path` tells the parser where to store the extracted text, metadata, and genetic sequences of articles.\\\n",
    "`txt_path` tells the parser where to store the text of articles.\n",
    "\n",
    "The differences between `output_path` and `txt_path` are: `output_path` contains pickle files that have all the information extracted from articles and supplementary files (text, metadata, genetic sequences), whereas `txt_path` contains txt files that only have the text of articles. We did this because storing the text of articles in txt format is easier to read for tasks that do not require metadata and sequence information. The text data in both paths are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "# the directory that stores our supplementary data dump\n",
    "# since the supplementary data dump also contains the original article\n",
    "# we can use this as the input path\n",
    "input_path = os.path.abspath(\"/home/jiawei/data/acs-data/original-files/\")\n",
    "\n",
    "# the directory that stores our processed pickle files\n",
    "output_path = os.path.abspath(\"/home/jiawei/data/acs-data/processed-files/\")\n",
    "\n",
    "# the directory that stores our json files\n",
    "json_path = os.path.abspath(\"/home/jiawei/data/acs-data/json-files/\")\n",
    "\n",
    "# we also provide extracted text in plain text format, this is the output path\n",
    "# of the plain text\n",
    "txt_path = os.path.abspath(\"/home/jiawei/data/acs-data/txt-files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Parameters\n",
    "The parser also has some additional parameters:\\\n",
    "`valid_cache_path` points to a pickle files that stores the result of sbol api queries. This is to save time (api query is very slow over the network) for repeated run/debug.\\\n",
    "`non_acs_article_path` points to a txt files that has a list of publication numbers. Articles having these publication numbers are non-research article. \n",
    "\n",
    "**Important**: `non_acs_article_path` is deprecated because the list is not accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local file parameters don't change unless you know what you are doing\n",
    "\n",
    "# sbol api query cache, use to speed up the process time\n",
    "valid_cache_path = os.path.abspath(\"./sbol-validation-cache.pkl\")\n",
    "\n",
    "# deprecated, the list is inaccurate (false negative, non research article not in the list)\n",
    "non_acs_article_path = os.path.abspath(\"./non-acs-article.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBOL API\n",
    "The [sbol api](http://synbiodex.github.io/SBOL-Validator/#introduction) is an online service provided by [Myers Research Group at University of Utah](https://async.ece.utah.edu/tools/sbol-validatorconverter/). It takes any file, checks if the file is a *valid sequence file* (meaning: a file contains sequences information and adheres to one of the many popular computer-readable formats), and converts the sequence file into one of the following four formats (fasta, genbank, sbol, sbol2). We utilize this api in two ways: We use it to check whether a supplementary file is a valid sequence file, and convert a valid sequence file in any format to [fasta format](https://en.wikipedia.org/wiki/FASTA_format).\n",
    "\n",
    "Through an inspection at the supplementary files, we found that files that have extension like \".gb\", \".sbol\", \".fasta\", and \".dna\" are valid sequence files, \".txt\" supplementary files can be genbank files, and \".xml\" supplementary files can be sbol files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbol api request params\n",
    "sbol_validator_url = \"https://validator.sbolstandard.org/validate/\"\n",
    "\n",
    "# these are the file format \n",
    "sbol_allowed_file_type = set([\n",
    "    \"gb\", \"fasta\", \"sbol\", \"txt\", \"xml\", \"dna\"\n",
    "])\n",
    "validator_param = {\n",
    "    'options': {\n",
    "        'language' : \"FASTA\",\n",
    "        'test_equality': False,\n",
    "        'check_uri_compliance': False,\n",
    "        'check_completeness': False,\n",
    "        'check_best_practices': False,\n",
    "        'fail_on_first_error': True,\n",
    "        'provide_detailed_stack_trace': False,\n",
    "        'subset_uri': '',\n",
    "        'uri_prefix': 'dummy', # we need to have something here or the api will return an error\n",
    "        'version': '',\n",
    "        'insert_type': False\n",
    "    },\n",
    "    \"main_file\": None,\n",
    "    \"return_file\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text and Metadata Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1: Helper Functions for Extraction\n",
    "\n",
    "**Note**: To better understand the code below, I strongly suggest you to first read [lxml documentation](https://lxml.de/tutorial.html) and [XPath 1.0 documentation](https://www.w3.org/TR/1999/REC-xpath-19991116/), and open an article xml file on the side when going through this section (I used sb9b00030.xml as a reference when writing this documentation).\n",
    "\n",
    "The following code cells are many helper functions help the text and metadata extraction process. Currently, the parser have the following features:\n",
    "\n",
    "- **Extract Text:** The text of the articles are wrapped in `<p>` and `<title>` tags inside the xml files following the xpath `/article/body`. We use `extract_body()` to find those tags. An example is shown below:\n",
    "```\n",
    "<p>\n",
    "    Spontaneous and random back and forth switching between states \n",
    "    is certainly not suitable for processes requiring irreversible\n",
    "    cell fate determination, such as development and cell \n",
    "    differentiation. In these contexts, the intracellular noise \n",
    "    would need to be low enough to avoid spontaneous state \n",
    "    switching. To identify possible ratio control strategies in \n",
    "    such a low-noise environment, we transitioned experimentally \n",
    "    to a less-noisy system: the chromosomally integrated mutual \n",
    "    inhibition toggle in\n",
    "    <italic toggle=\"yes\">S. cerevisiae</italic>\n",
    "    .\n",
    "    <xref rid=\"ref25\" ref-type=\"bibr\"/>\n",
    "    This circuit has the same topology as that shown in\n",
    "    <xref rid=\"fig1\" ref-type=\"fig\">\n",
    "    Figure\n",
    "    <xref rid=\"fig1\" ref-type=\"fig\"/>\n",
    "    </xref>\n",
    "    a, exhibits hysteretic behavior (\n",
    "    <xref rid=\"fig2\" ref-type=\"fig\">\n",
    "    Figure\n",
    "    <xref rid=\"fig2\" ref-type=\"fig\"/>\n",
    "    </xref>\n",
    "    a), and favors the TetR-dominant, low-GFP state under no induction. \n",
    "    However, differences in promoters, copy number, and transcription\n",
    "    –translation processes between\n",
    "    <italic toggle=\"yes\">E. coli</italic>\n",
    "    and yeast serve to reduce intracellular noise and shift the \n",
    "    bistable region up to roughly 3–17 ng/mL ATc. Unlike in\n",
    "    <italic toggle=\"yes\">E. coli</italic>\n",
    "    , the bulk of the bistable range was impervious to the effects \n",
    "    of intracellular noise, resulting in a single peak homogeneous \n",
    "    expression profile even when the system is operating within the \n",
    "    bistable region.\n",
    "</p>\n",
    "```\n",
    "In general the text contains formatting tags, embedded references, and captions. Our goal is to remove these and extract the plain text. To do this, we implement `p_helper()` to retrieve the plain text in fragments, join them together into one line, and fix punctuation/parenthesis placement. We call this one line of sanitized text returned by `p_helper()` a *paragraph*. `title_helper()` is used to retrive the title in plain text. In addition to extracting paragraphs and titles, we also use the titles to label what section (intro, method, results, discussion, and so on) the paragraphs are in. The logic is coded in `extract_body()`: If we see a paragraph following a title containing \"method\", we indicate the paragraph is in \"method\" section.\n",
    "\n",
    "\n",
    "- **Extract Abstract:** Abstract is found in tag `<abstract>`. It is wrapped in a `<p>` tag similar to a paragraph in the body. We use `extract_abstract()` to locate the tag and `p_helper()` to extract the plain text.\n",
    "\n",
    "\n",
    "- **Extract Keywords:** Keywords can be found in tag `<kwd-group>`. `extract_keywords()` and `kwd_helper()` is used to locate and retrieve the keywords in the article.\n",
    "\n",
    "\n",
    "- **Extract Publication Dates:** There are two type of dates we can find in the metadata, one is electronic publication date and the other is issue puiblication date, they are stored in `<pub-date>` tags. You can tell which is the electronic publication date by looking at the attributes of the tags. `extract_date()` handles dates extraction.\n",
    "\n",
    "\n",
    "- **Extract History of Events:** Stored in `<history>`, history of events is a list of event the article has been through. Each entry in the list contains the name of event and the date of the event. `extract_history()` handles history extraction.\n",
    "\n",
    "\n",
    "- **Extract Article Type:** Located in `<subj-group>`, It indicate the type of the article, and we are currently using this to determine if an article is research article or not. `extract_article_type()` handles this process.\n",
    "\n",
    "\n",
    "- **Extract id and internal id:** Overall, we can find three different ids that can uniquely identify an article: 1) the publication number (mentioned in \"Input Data\" section above), 2) the id in `<article-id>`, and 3) the internal id in the attribute of `<article>`. We believe 3) is an id used internally by ACS because we cannot find any public available information using this id.\n",
    "\n",
    "\n",
    "- **Extract Article Title:** Located in `<article-title>`, it contains the title of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect matching files and dirs\n",
    "# basically a fancy find method\n",
    "# pattern is a re pattern\n",
    "# collect_dirs is a bool, when set to true it will collect directories/folders\n",
    "# return a list of files/directories that matches the pattern\n",
    "def collect_files(root, res, pattern=\"\", collect_dirs=True, min_depth=None, max_depth=None):\n",
    "    \n",
    "    # check max depth\n",
    "    if not max_depth is None and max_depth == 0:\n",
    "        return\n",
    "    \n",
    "    # go through all item in the dir\n",
    "    for item in os.listdir(root):\n",
    "        \n",
    "        # process item\n",
    "        item_path = os.path.join(root, item)\n",
    "        item_is_dir = os.path.isdir(item_path)\n",
    "        \n",
    "        # put valid file in res if min depth has reached\n",
    "        if min_depth is None or min_depth - 1 <= 0:\n",
    "            if re.match(pattern, item_path):\n",
    "                if not item_is_dir or collect_dirs:\n",
    "                    res.append(item_path)\n",
    "        \n",
    "        # recursively collect all files\n",
    "        if item_is_dir:\n",
    "            next_min_depth = None if min_depth is None else min_depth - 1\n",
    "            next_max_depth = None if max_depth is None else max_depth - 1\n",
    "            collect_files(item_path, res, pattern, collect_dirs, next_min_depth, next_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helps to extract text from paragraph\n",
    "def p_helper(node):\n",
    "    \n",
    "    # <p/> does not have text\n",
    "    if node.text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # each paragarph is put into a line\n",
    "    line_list = [node.text]\n",
    "    for child in node:\n",
    "\n",
    "        # get the text inside the child if the tag isn't \n",
    "        # named-content and inline-formula (those are mathametical formulas)\n",
    "        # and the text following the child\n",
    "        if not child.tag in (\"named-content\", \"inline-formula\"):\n",
    "            line_list.append(\" \".join(child.xpath(\".//text()\")))\n",
    "        line_list.append(child.tail)\n",
    "\n",
    "    # there might be none in line_list\n",
    "        \n",
    "    # re dark magic below\n",
    "    # remove new line and spaces\n",
    "    line = \" \".join(line_list)\n",
    "    line = line.strip()\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "\n",
    "    # clean up consecutive spaces\n",
    "    line = re.sub(\"\\s+\", \" \", line)\n",
    "\n",
    "    # fix the space around punctuation\n",
    "    line = re.sub(\"\\s([.,\\):;])\", r\"\\1\", line)\n",
    "    line = re.sub(\"\\(\\s\", r\"(\", line)\n",
    "    line = re.sub(\"\\s*([-/])\\s*\", r\"\\1\", line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip format from keyword nodes\n",
    "def kwd_helper(node):\n",
    "    \n",
    "    # return a keyword string\n",
    "    kwd_tokens = node.xpath(\".//text()\")\n",
    "    kwd = \" \".join(kwd_tokens).replace(\"\\n\", \" \").strip()\n",
    "    kwd = re.sub(\"\\s+\", \" \", kwd)\n",
    "    return kwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns interesting titles\n",
    "# for example: intro, method, and results\n",
    "# return None for non interesting titles\n",
    "def title_helper(node):\n",
    "    \n",
    "    # extract text from title node\n",
    "    title = \" \".join(node.xpath(\".//text()\"))\n",
    "    title = title.replace(\"\\n\", \" \")\n",
    "    title = re.sub(\"\\s+\", \" \", title)\n",
    "    title = title.strip()\n",
    "    original = title\n",
    "    title = title.lower()\n",
    "    \n",
    "    # categorize title\n",
    "    normalized = []\n",
    "    if \"intro\" in title:\n",
    "        normalized.append(\"introduction\")\n",
    "    if \"result\" in title:\n",
    "        normalized.append(\"result\")\n",
    "    if \"discuss\" in title:\n",
    "        normalized.append(\"discussion\")\n",
    "    if \"material\" in title:\n",
    "        normalized.append(\"materials\")\n",
    "    if \"method\" in title or \"procedure\" in title:\n",
    "        normalized.append(\"method\")\n",
    "    if \"summary\" in title:\n",
    "        normalized.append(\"summary\")\n",
    "    return normalized, original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from xpath nodes\n",
    "def extract_body(root):\n",
    "    \n",
    "    # we are interested in the text in the body section\n",
    "    curr_normalized = []\n",
    "    curr_original = None\n",
    "    text = []\n",
    "    text_nodes = root.xpath(\"/article/body//*[self::p or (self::title and not(ancestor::caption))]\")\n",
    "    for text_node in text_nodes:\n",
    "        \n",
    "        # handle title\n",
    "        if text_node.tag == \"title\":\n",
    "            normalized, original = title_helper(text_node)\n",
    "            curr_original = original\n",
    "            if len(normalized) > 0:\n",
    "                curr_normalized = normalized\n",
    "            text.append({\n",
    "                \"text\": original,\n",
    "                \"normalized_header\": curr_normalized,\n",
    "                \"section_header\": curr_original,\n",
    "                \"type\": \"title\"\n",
    "            })\n",
    "        \n",
    "        # handle paragraph\n",
    "        elif text_node.tag == \"p\":\n",
    "            text.append({\n",
    "                \"text\": p_helper(text_node),\n",
    "                \"normalized_section_header\": curr_normalized,\n",
    "                \"section_header\":  curr_original,\n",
    "                \"type\": \"paragraph\"\n",
    "            })\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract abstract\n",
    "def extract_abstract(root):\n",
    "    \n",
    "    # get the abstract paragraph\n",
    "    abstract = []\n",
    "    abstract_nodes = root.xpath(\"//abstract/p\")\n",
    "    if abstract_nodes:\n",
    "        abstract.append(p_helper(abstract_nodes[0]))\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_keywords from meta data\n",
    "def extract_keywords(root):\n",
    "    \n",
    "    # get the keywords\n",
    "    keywords = []\n",
    "    kwd_nodes = root.xpath(\"//kwd-group/kwd\")\n",
    "    for kwd_node in kwd_nodes:\n",
    "        keywords.append(kwd_helper(kwd_node))\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date information from meta data\n",
    "def extract_date(root):\n",
    "    \n",
    "    issue_pub_date = None\n",
    "    electron_pub_date = None\n",
    "    \n",
    "    # traverse to the date note\n",
    "    date_nodes = root.xpath(\"/article/front/article-meta/pub-date\")\n",
    "    \n",
    "    # get the time\n",
    "    for node in date_nodes:\n",
    "        year = node.xpath(\"./year\")[0].text.strip()\n",
    "        month = node.xpath(\"./month\")[0].text.strip()\n",
    "        day = node.xpath(\"./day\")[0].text.strip()\n",
    "\n",
    "        if \"date-type\" in node.attrib and node.attrib[\"date-type\"] == \"issue-pub\":\n",
    "            issue_pub_date = \"%s/%s/%s\" % (month, day, year)\n",
    "        else:\n",
    "            electron_pub_date = \"%s/%s/%s\" % (month, day, year)\n",
    "    \n",
    "    return issue_pub_date, electron_pub_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract article-id from meta data\n",
    "def extract_id(root):\n",
    "    \n",
    "    id_node = root.xpath(\"/article/front/article-meta/article-id\")[0]\n",
    "    article_id = id_node.text.strip()\n",
    "    return article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract internal id, the one in the first line of the xml file\n",
    "def extract_internal_id(root):\n",
    "    \n",
    "    id_node = root.xpath(\"/article\")[0]\n",
    "    return id_node.attrib[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a list of history from the meta data\n",
    "def extract_history(root):\n",
    "    \n",
    "    res = []\n",
    "    dates = root.xpath(\"/article/front/article-meta/history/date\")\n",
    "    for date in dates:\n",
    "        year = date.xpath(\"./year\")[0].text.strip()\n",
    "        month = date.xpath(\"./month\")[0].text.strip()\n",
    "        day = date.xpath(\"./day\")[0].text.strip()\n",
    "        res.append({\n",
    "            \"event\": date.attrib[\"date-type\"],\n",
    "            \"time\": \"%s/%s/%s\" % (month, day, year)\n",
    "        })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the subject of the article from meta data\n",
    "def extract_article_type(root):\n",
    "    article_type = root.xpath(\"/article/front/article-meta/article-categories/subj-group\")\n",
    "    if len(article_type) > 1:\n",
    "        print(\"article have 2 or more types\")\n",
    "    raw = article_type[0].xpath(\".//text()\")\n",
    "    # clean up\n",
    "    res = ''.join(raw).strip()\n",
    "    res = re.sub(\"\\n\", \"\", res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_title(root):\n",
    "    article_title = root.xpath(\"/article/front/article-meta/title-group/article-title\")\n",
    "    return p_helper(article_title[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: Extraction Procedure\n",
    "\n",
    "1. **Collect xml Files:** `colect_files()` is used to find all the article xml files. Notice we are only collecting xml file that is one directory deep because the suppl folder also contains \n",
    "2. **Extract Information:** We open each xml file, and use the helper methods above to extract information we want.\n",
    "\n",
    "All information is stored in a dictionary call `processed_files`. It maps publication numbers to extracted information from articles. We will refer the values of this dictionary as *article info*. For example `processed_files[\"sb9b00030\"]` returns the article info retrieved from sb9b00030.xml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all xml files\n",
    "xml_paths = []\n",
    "collect_files(input_path, xml_paths, pattern=\".*\\.xml$\", collect_dirs=False, min_depth=1, max_depth=2)\n",
    "print(f\"total xml files: %d\" % len(xml_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated: get a list of non research article\n",
    "# non_acs_article_list = set()\n",
    "# with open(non_acs_article_path, \"r\") as f:\n",
    "#     for line in f:\n",
    "#         non_acs_article_list.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the files\n",
    "# after this block, the information will be extracted to var processeed_files\n",
    "# processed_files: key(pub_num) -> value(article_info)\n",
    "processed_files = {}\n",
    "for xml_path in tqdm(xml_paths):\n",
    "\n",
    "    # print(\"\\nparsing %s\" % xml_path)\n",
    "    pub_num = xml_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    # get the root of the xml\n",
    "    root = etree.parse(xml_path).getroot()\n",
    "    \n",
    "    # get the pub date\n",
    "    issue_pub_date, electron_pub_date = extract_date(root)\n",
    "    \n",
    "    # get the article type\n",
    "    article_type = extract_article_type(root)\n",
    "\n",
    "    # create a dictionary holding the extracted data\n",
    "    xml_data = {\n",
    "        \"pub_num\": pub_num,\n",
    "        \"is_research\": \"research\" in article_type.lower(),\n",
    "        \"keywords\": extract_keywords(root),\n",
    "        \"abstract\": extract_abstract(root),\n",
    "        \"body\": extract_body(root),\n",
    "        \"issue_pub_date\": issue_pub_date,\n",
    "        \"electron_pub_date\": electron_pub_date,\n",
    "        \"article_id\": extract_id(root),\n",
    "        \"internal_id\": extract_internal_id(root),\n",
    "        \"history\": extract_history(root),\n",
    "        \"type\": article_type,\n",
    "        \"article_title\": extract_article_title(root),\n",
    "    }\n",
    "\n",
    "    # save the data\n",
    "    processed_files[pub_num] = xml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print one sample for inspection\n",
    "pprint(processed_files[\"sb9b00030\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some stats\n",
    "type_count = {}\n",
    "event_count = {}\n",
    "for k, article_info in processed_files.items():\n",
    "    article_type = article_info[\"type\"]\n",
    "    if not article_type in type_count:\n",
    "        type_count[article_type] = 0\n",
    "    type_count[article_type] += 1\n",
    "    \n",
    "    events = article_info[\"history\"]\n",
    "    for e in events:\n",
    "        if not e[\"event\"] in event_count:\n",
    "            event_count[e[\"event\"]] = 0\n",
    "        event_count[e[\"event\"]] += 1\n",
    "pprint(type_count)\n",
    "print()\n",
    "pprint(event_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Genetic Sequences Extraction\n",
    "\n",
    "For this part we want to go through all the supplementary files in the data dump and recover genetic sequences from them. This requires handling of different file type and using the sbol api. Once we extracted the genetic sequences, we will store the sequence data in fasta format in `processed_files[pub_num][suppl_files][sequences]` for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1: Unzipping\n",
    "\n",
    "Some supplementary files are compressed into zip files, we want to unzip them first before doing anything. Because we only need to do it once, the code below is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out all the zip files and unzip them\n",
    "# do this once when there is new data\n",
    "# suppl_files_zip = []\n",
    "# collect_files(input_path, suppl_files_zip, pattern=\".*\\.zip$\", collect_dirs=False, min_depth=3)\n",
    "# print(\"zip files: %d\" % len(suppl_files_zip))\n",
    "# for zip_file in suppl_files_zip:\n",
    "#     zip_file_dir = re.sub(\"/[^/]*$\", \"\", zip_file)\n",
    "#     res = sp.run([\"unzip\", \"-n\", zip_file, \"-d\", zip_file_dir])\n",
    "#     if res.returncode != 0:\n",
    "#         print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: Finding Paths to All Supplementary Files\n",
    "\n",
    "Unlike the locations of article xml files, the structures of suppl folder is unpreditable. Therefore we need to recursively find all the files in the suppl folder. We store the absolute path to all supplementary files in a list called `suppl_files_all`. After we have the list, we need to put this information into the `processed_file` so that we can achive two goals: 1) to query supplementary files using publication number, and 2) to locate supplementary files relative to `input_path` using publication number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all suppl files\n",
    "suppl_files_all = []\n",
    "collect_files(input_path, suppl_files_all, pattern=\"\", collect_dirs=False, min_depth=3)\n",
    "suppl_files_all = [x for x in suppl_files_all if not re.match(\".*__MACOSX.*\", x)]\n",
    "print(\"suppl files: %d\" % len(suppl_files_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach all the suppl path to processed files\n",
    "# clear the suppl list of each article first to make the following code idempotent\n",
    "for article_info in processed_files.values():\n",
    "    article_info[\"suppl_files\"] = []\n",
    "    \n",
    "for path in suppl_files_all:\n",
    "    path = path.split(\"/\")\n",
    "    \n",
    "    # get basic attrib\n",
    "    suppl_filename = path[-1]\n",
    "    suppl_dir = input_path.split(\"/\")[-1]\n",
    "    suppl_dir_idx = 0\n",
    "    for i, item in enumerate(path):\n",
    "        if item == suppl_dir:\n",
    "            suppl_dir_idx = i\n",
    "            break\n",
    "    else:\n",
    "        assert False # we should not be here\n",
    "    pub_num = path[suppl_dir_idx + 1]\n",
    "    rpath = os.path.join(*path[suppl_dir_idx + 1:]) # relative path\n",
    "    \n",
    "    # create info dict\n",
    "    suppl_info = {\n",
    "        \"suppl_filename\": suppl_filename,\n",
    "        \"rpath\": rpath,\n",
    "        \"sequences\": None\n",
    "    }\n",
    "    \n",
    "    # push it into the processed files dict\n",
    "    if pub_num in processed_files:\n",
    "        processed_files[pub_num][\"suppl_files\"].append(suppl_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3 SOBL API Validation\n",
    "\n",
    "In this part, we use sbol api to find all the valid sequence files and extract the sequence into fasta format. There are some limitation on the sbol api: 1) the reponse time over network is long and 2) the api only accept file size up to 64MB. To address this problem, currently we are ignoring large files and maintaining a cache locally to increase the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get supplementary files by extension\n",
    "def filter_suppl_file_by_ext(all_articles, allowed_ext):\n",
    "    suppl_files_to_check = []\n",
    "    for article_info in all_articles.values():\n",
    "        for suppl_info in article_info[\"suppl_files\"]:\n",
    "            # only allow the following extension\n",
    "            ext = suppl_info[\"suppl_filename\"].split(\".\")[-1]\n",
    "            if ext in allowed_ext:\n",
    "                suppl_files_to_check.append((suppl_info, article_info))\n",
    "    return suppl_files_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sbol api to validate sequence files\n",
    "def validate_sequence(file):\n",
    "    \n",
    "    # restrict file size to be less than 64mb\n",
    "    # this is an api restriction\n",
    "    file_size = os.path.getsize(file)\n",
    "    if file_size >= 64 * 2 ** 20:\n",
    "        return False, None\n",
    "    \n",
    "    # try to read the content\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        return False, None\n",
    "    \n",
    "    # validate file\n",
    "    validator_param[\"main_file\"] = content\n",
    "    res = requests.post(sbol_validator_url, json=validator_param).json()\n",
    "    return res[\"valid\"], res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the api to check if the file is a sequence file\n",
    "# we will cache the request result to reduce server load\n",
    "valid_res_cache = None\n",
    "if os.path.exists(valid_cache_path):\n",
    "    with open(valid_cache_path, \"rb\") as f:\n",
    "        valid_res_cache = pickle.load(f)\n",
    "else:\n",
    "    valid_res_cache = {}\n",
    "\n",
    "# create a list of all the suppl files that we need to check\n",
    "sbol_suppl_files_to_check = filter_suppl_file_by_ext(processed_files, sbol_allowed_file_type)\n",
    "\n",
    "for suppl_info, article_info in tqdm(sbol_suppl_files_to_check):\n",
    "\n",
    "    # use api or cache to get convert and store the sequence file in fasta format\n",
    "    path = os.path.join(input_path, suppl_info[\"rpath\"])\n",
    "    if not suppl_info[\"rpath\"] in valid_res_cache:\n",
    "        is_valid, data = validate_sequence(path)\n",
    "        valid_res_cache[suppl_info[\"rpath\"]] = (is_valid, data)\n",
    "    else:\n",
    "        is_valid, data = valid_res_cache[suppl_info[\"rpath\"]]\n",
    "    suppl_info[\"sequences\"] = data[\"result\"] if is_valid else None\n",
    "\n",
    "# save cache\n",
    "with open(valid_cache_path, \"wb\") as f:\n",
    "    pickle.dump(valid_res_cache, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.4 Extract Sequences from PDF\n",
    "\n",
    "Most of the supplementary files are not valid sequence files, but a lot of them are pdf files and contain sequences. In this part we try to use the pdftotext to convert the pdf to plain text first, and then salvage sequences from the plain text.\n",
    "\n",
    "When a sequence in pdf is converted to plain text, two thing can happend: 1) the sequence is splited across multiple lines, or 2) the sequence remain in one line. The script extract the multiline sequences first and then replace them with a place holder and then extract the single line sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds any strings that looks like a sequence in pdf\n",
    "# need to have pdftotext installed\n",
    "def extract_sequence_from_pdf(path):\n",
    "    sp.run([\"pdftotext\", \"-raw\", path, \"/tmp/sbks-pdf-tmp.txt\"])\n",
    "    seqs = []\n",
    "    with open(\"/tmp/sbks-pdf-tmp.txt\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        text = \"\".join(lines).upper()\n",
    "        \n",
    "        # first find all the multiline sequences\n",
    "        for multi_line_seq in re.findall(\"([ATCG]{3,}(\\n[ATCG]{3,}){1,})\", text):\n",
    "            seqs.append(re.sub(\"\\n\", \"\", multi_line_seq[0]))\n",
    "            \n",
    "        # remove extracted sequences to avoid double counting\n",
    "        text = re.sub(\"([ATCG]{3,}(\\n[ATCG]{3,}){1,})\", \" ###REMOVED### \", text)\n",
    "        \n",
    "        # then find all the one line sequences\n",
    "        for single_line_seq in re.findall(\"[ATCG]{10,}\", text):\n",
    "            seqs.append(single_line_seq)\n",
    "            \n",
    "#     sp.run([\"rm\", \"tmp.txt\"])\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sequences from pdf\n",
    "# create a list of all the suppl files that we need to check\n",
    "pdf_suppl_files_to_check = filter_suppl_file_by_ext(processed_files, set([\"pdf\"]))\n",
    "\n",
    "for suppl_info, article_info in tqdm(pdf_suppl_files_to_check):\n",
    "\n",
    "    # use pdftotext to extract the sequences from pdf\n",
    "    path = os.path.join(input_path, suppl_info[\"rpath\"])\n",
    "    sequences = extract_sequence_from_pdf(path)\n",
    "    if sequences:\n",
    "        res = []\n",
    "        for i, s in enumerate(sequences):\n",
    "            res.append(f\">{article_info['internal_id']}_{suppl_info['suppl_filename']}_{i}\\n\")\n",
    "            res.append(s + \"\\n\")\n",
    "        suppl_info[\"sequences\"] = \"\".join(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.4 Extract Sequences from Word document\n",
    "\n",
    "There are sequences in the word document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sequence_from_docx(path):\n",
    "    seqs = []\n",
    "    text = []\n",
    "    doc = Document(path)\n",
    "    for para in doc.paragraphs:\n",
    "        text.append(para.text)\n",
    "    text = \"\\n\\n\".join(text)\n",
    "    # first find all the multiline sequences\n",
    "    for multi_line_seq in re.findall(\"([ATCG]{3,}(\\n[ATCG]{3,}){1,})\", text):\n",
    "        seqs.append(re.sub(\"\\n\", \"\", multi_line_seq[0]))\n",
    "\n",
    "    # remove extracted sequences to avoid double counting\n",
    "    text = re.sub(\"([ATCG]{3,}(\\n[ATCG]{3,}){1,})\", \" ###REMOVED### \", text)\n",
    "\n",
    "    # then find all the one line sequences\n",
    "    for single_line_seq in re.findall(\"[ATCG]{10,}\", text):\n",
    "        seqs.append(single_line_seq)\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_suppl_files_to_check = filter_suppl_file_by_ext(processed_files, set([\"docx\", \"doc\"]))\n",
    "\n",
    "for suppl_info, article_info in tqdm(pdf_suppl_files_to_check):\n",
    "\n",
    "    path = os.path.join(input_path, suppl_info[\"rpath\"])\n",
    "    sequences = extract_sequence_from_docx(path)\n",
    "    if sequences:\n",
    "        res = []\n",
    "        for i, s in enumerate(sequences):\n",
    "            res.append(f\">{article_info['internal_id']}_{suppl_info['suppl_filename']}_{i}\\n\")\n",
    "            res.append(s + \"\\n\")\n",
    "        suppl_info[\"sequences\"] = \"\".join(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Writing Results\n",
    "\n",
    "There are three things written to the disk:\n",
    "- **Processed Files:** Each entry is saved into one pickle file and named using the publication number. The pickle files are saved in `output_path`.\n",
    "- **Plain Text:** The text data of each article is saved into one txt file and named using the publication number. The txt files are saved in `txt_path`.\n",
    "- **sequences:** The sequences extracted from each article is saved into one fasta formatted txt file. They are saved in `./sequence-files` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the sequence files\n",
    "for pub_num, article_info in processed_files.items():\n",
    "    seq_to_write = []\n",
    "    for suppl_info in article_info[\"suppl_files\"]:\n",
    "        if not suppl_info[\"sequences\"] is None:\n",
    "            seq_to_write.append(suppl_info[\"sequences\"])\n",
    "    if len(seq_to_write) > 0:\n",
    "        with open(os.path.join(\"sequence-files\", pub_num + \".seq.txt\"), \"w\") as outfile:\n",
    "            for seq in seq_to_write:\n",
    "                outfile.write(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle and json and txt the processed files\n",
    "for pub_num, data in tqdm(processed_files.items()):\n",
    "    with open(os.path.join(output_path, pub_num + \".pkl\"), \"wb\") as out:\n",
    "        pickle.dump(data, out)\n",
    "        \n",
    "    with open(os.path.join(json_path, pub_num + \".json\"), \"w\") as out:\n",
    "        json.dump(data, out, indent=4)\n",
    "        \n",
    "    text_data = [d[\"text\"] + \"\\n\" for d in data[\"body\"]]\n",
    "    \n",
    "    # save plain text\n",
    "    if data[\"is_research\"]:\n",
    "        text_data_path = os.path.join(txt_path, \"research\")\n",
    "    else:\n",
    "        text_data_path = os.path.join(txt_path, \"non-research\")\n",
    "    with open(os.path.join(text_data_path, pub_num + \".txt\"), \"w\") as out:\n",
    "        out.writelines(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check one pickle\n",
    "with open(os.path.join(output_path, \"sb9b00030.pkl\"), \"rb\") as ifile:\n",
    "    article_info = pickle.load(ifile)\n",
    "    pprint(article_info.keys())\n",
    "    pprint(article_info[\"type\"])\n",
    "    pprint(article_info[\"is_research\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Notes: There are 826 articles have sequences files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
