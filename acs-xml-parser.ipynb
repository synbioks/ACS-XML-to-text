{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import requests\n",
    "import pickle\n",
    "import getopt\n",
    "import subprocess as sp\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from lxml import etree\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "input_path = os.path.abspath(\"/mnt/data1/jiawei/acs-data/article-files/\")\n",
    "output_path = os.path.abspath(\"/mnt/data1/jiawei/acs-data/processed-files/\")\n",
    "txt_path = os.path.abspath(\"/mnt/data1/jiawei/acs-data/txt-files\")\n",
    "suppl_path = os.path.abspath(\"/mnt/data1/jiawei/acs-data/suppl-files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local file parameters\n",
    "valid_cache_path = os.path.abspath(\"./sbol-validation-cache.pkl\")\n",
    "non_acs_article_path = os.path.abspath(\"./non-acs-article.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect matching files and dirs\n",
    "def collect_files(root, res, pattern=\"\", collect_dirs=True, min_depth=None, max_depth=None):\n",
    "    \n",
    "    # check max depth\n",
    "    if not max_depth is None and max_depth == 0:\n",
    "        return\n",
    "    \n",
    "    # go through all item in the dir\n",
    "    for item in os.listdir(root):\n",
    "        \n",
    "        # process item\n",
    "        item_path = os.path.join(root, item)\n",
    "        item_is_dir = os.path.isdir(item_path)\n",
    "        \n",
    "        # pull valid file in res if min depth has reached\n",
    "        if min_depth is None or min_depth - 1 <= 0:\n",
    "            if re.match(pattern, item_path):\n",
    "                if not item_is_dir or collect_dirs:\n",
    "                    res.append(item_path)\n",
    "        \n",
    "        # recursively collect all files\n",
    "        if item_is_dir:\n",
    "            next_min_depth = None if min_depth is None else min_depth - 1\n",
    "            next_max_depth = None if max_depth is None else max_depth - 1\n",
    "            collect_files(item_path, res, pattern, collect_dirs, next_min_depth, next_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helps to extract text from paragraph\n",
    "def p_helper(node):\n",
    "    \n",
    "    # <p/> does not have text\n",
    "    if node.text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # each paragarph is put into a line\n",
    "    line_list = [node.text]\n",
    "    for child in node:\n",
    "\n",
    "        # get the text inside the child if the tag isn't \n",
    "        # named-content and inline-formula\n",
    "        # and the text following the child\n",
    "        if not child.tag in (\"named-content\", \"inline-formula\"):\n",
    "            line_list.append(\" \".join(child.xpath(\".//text()\")))\n",
    "        line_list.append(child.tail)\n",
    "\n",
    "    # there might be none in line_list\n",
    "        \n",
    "    # re dark magic\n",
    "    # remove new line and spaces\n",
    "    line = \" \".join(line_list)\n",
    "    line = line.strip()\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "\n",
    "    # clean up consecutive spaces\n",
    "    line = re.sub(\"\\s+\", \" \", line)\n",
    "\n",
    "    # fix the space around punctuation\n",
    "    line = re.sub(\"\\s([.,\\):;])\", r\"\\1\", line)\n",
    "    line = re.sub(\"\\(\\s\", r\"(\", line)\n",
    "    line = re.sub(\"\\s*([-/])\\s*\", r\"\\1\", line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kwd_helper(node):\n",
    "    \n",
    "    # return a keyword string\n",
    "    kwd_tokens = node.xpath(\".//text()\")\n",
    "    kwd = \" \".join(kwd_tokens).replace(\"\\n\", \" \").strip()\n",
    "    kwd = re.sub(\"\\s+\", \" \", kwd)\n",
    "    return kwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns interesting titles\n",
    "# for example: intro, method, and results\n",
    "# return None for non interesting titles\n",
    "def title_helper(node):\n",
    "    \n",
    "    # extract text from title node\n",
    "    title = \" \".join(node.xpath(\".//text()\"))\n",
    "    title = title.replace(\"\\n\", \" \")\n",
    "    title = re.sub(\"\\s+\", \" \", title)\n",
    "    title = title.strip()\n",
    "    title = title.lower()\n",
    "    \n",
    "    # categorize title\n",
    "    res = []\n",
    "    if \"intro\" in title:\n",
    "        res.append(\"introduction\")\n",
    "    if \"result\" in title:\n",
    "        res.append(\"result\")\n",
    "    if \"discuss\" in title:\n",
    "        res.append(\"discussion\")\n",
    "    if \"material\" in title:\n",
    "        res.append(\"materials\")\n",
    "    if \"method\" in title or \"procedure\" in title:\n",
    "        res.append(\"method\")\n",
    "    if \"summary\" in title:\n",
    "        res.append(\"summary\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_body(root):\n",
    "    \n",
    "    # we are interested in the text in the body section\n",
    "    curr_title = []\n",
    "    text = []\n",
    "    text_nodes = root.xpath(\"/article/body//*[self::p or (self::title and not(ancestor::caption))]\")\n",
    "    for text_node in text_nodes:\n",
    "        \n",
    "        # handle title\n",
    "        if text_node.tag == \"title\":\n",
    "            tmp_title = title_helper(text_node)\n",
    "            if len(tmp_title) > 0:\n",
    "                curr_title = tmp_title\n",
    "        \n",
    "        # handle paragraph\n",
    "        elif text_node.tag == \"p\":\n",
    "            text.append({\n",
    "                \"text\": p_helper(text_node),\n",
    "                \"section\": curr_title\n",
    "            })\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abstract(root):\n",
    "    \n",
    "    # get the abstract paragraph\n",
    "    abstract = []\n",
    "    abstract_nodes = root.xpath(\"//abstract/p\")\n",
    "    if abstract_nodes:\n",
    "        abstract.append(p_helper(abstract_nodes[0]))\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(root):\n",
    "    \n",
    "    # get the keywords\n",
    "    keywords = []\n",
    "    kwd_nodes = root.xpath(\"//kwd-group/kwd\")\n",
    "    for kwd_node in kwd_nodes:\n",
    "        keywords.append(kwd_helper(kwd_node))\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(root):\n",
    "    \n",
    "    issue_pub_date = None\n",
    "    electron_pub_date = None\n",
    "    \n",
    "    # traverse to the date note\n",
    "    date_nodes = root.xpath(\"/article/front/article-meta/pub-date\")\n",
    "    \n",
    "    # get the time\n",
    "    for node in date_nodes:\n",
    "        year = node.xpath(\"./year\")[0].text.strip()\n",
    "        month = node.xpath(\"./month\")[0].text.strip()\n",
    "        day = node.xpath(\"./day\")[0].text.strip()\n",
    "\n",
    "        if \"date-type\" in node.attrib and node.attrib[\"date-type\"] == \"issue-pub\":\n",
    "            issue_pub_date = \"%s/%s/%s\" % (month, day, year)\n",
    "        else:\n",
    "            electron_pub_date = \"%s/%s/%s\" % (month, day, year)\n",
    "    \n",
    "    return issue_pub_date, electron_pub_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all xml files\n",
    "xml_paths = []\n",
    "collect_files(input_path, xml_paths, pattern=\".*\\.xml$\", collect_dirs=False)\n",
    "print(f\"total xml files: %d\" % len(xml_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't parse non acs article\n",
    "non_acs_article_list = set()\n",
    "with open(non_acs_article_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        non_acs_article_list.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the files\n",
    "processed_files = {}\n",
    "for xml_path in tqdm(xml_paths):\n",
    "\n",
    "    # print(\"\\nparsing %s\" % xml_path)\n",
    "    pub_num = xml_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    # get the root of the xml\n",
    "    root = etree.parse(xml_path).getroot()\n",
    "    \n",
    "    # get the pub date\n",
    "    issue_pub_date, electron_pub_date = extract_date(root)\n",
    "\n",
    "    # create a dictionary holding the xml data\n",
    "    xml_data = {\n",
    "        \"is_research\": not pub_num in non_acs_article_list,\n",
    "        \"keywords\": extract_keywords(root),\n",
    "        \"abstract\": extract_abstract(root),\n",
    "        \"body\": extract_body(root),\n",
    "        \"issue_pub_date\": issue_pub_date,\n",
    "        \"electron_pub_date\": electron_pub_date,\n",
    "        \"suppl_files\": []\n",
    "    }\n",
    "\n",
    "    # save the data\n",
    "    processed_files[pub_num] = xml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(processed_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out all the zip files\n",
    "# suppl_files_zip = []\n",
    "# collect_files(suppl_path, suppl_files_zip, pattern=\".*\\.zip$\", collect_dirs=False, min_depth=3)\n",
    "# print(\"files: %d\" % len(suppl_files_zip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the zip files in place\n",
    "# for zip_file in suppl_files_zip:\n",
    "#     zip_file_dir = re.sub(\"/[^/]*$\", \"\", zip_file)\n",
    "#     res = sp.run([\"unzip\", \"-n\", zip_file, \"-d\", zip_file_dir])\n",
    "#     if res.returncode != 0:\n",
    "#         print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all suppl files\n",
    "suppl_files_all = []\n",
    "collect_files(suppl_path, suppl_files_all, pattern=\"\", collect_dirs=False, min_depth=3)\n",
    "suppl_files_all = [x for x in suppl_files_all if not re.match(\".*__MACOSX.*\", x)]\n",
    "print(\"files: %d\" % len(suppl_files_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach all the suppl path to processed files\n",
    "suppl_ext = set() # debug\n",
    "for path in suppl_files_all:\n",
    "    path = path.split(\"/\")\n",
    "    \n",
    "    # get basic attrib\n",
    "    suppl_filename = path[-1]\n",
    "    suppl_dir = suppl_path.split(\"/\")[-1]\n",
    "    suppl_dir_idx = 0\n",
    "    for i, item in enumerate(path):\n",
    "        if item == suppl_dir:\n",
    "            suppl_dir_idx = i\n",
    "            break\n",
    "    else:\n",
    "        assert False # we should not be here\n",
    "    pub_num = path[suppl_dir_idx + 1]\n",
    "    rpath = os.path.join(*path[suppl_dir_idx + 1:]) # relative path\n",
    "    \n",
    "    # create info dict\n",
    "    suppl_info = {\n",
    "        \"suppl_filename\": suppl_filename,\n",
    "        \"rpath\": rpath,\n",
    "        \"is_sequence\": False\n",
    "    }\n",
    "    \n",
    "    # push it into the processed files dict\n",
    "    if pub_num in processed_files:\n",
    "        processed_files[pub_num][\"suppl_files\"].append(suppl_info)\n",
    "    \n",
    "    # collect file extension (for debug)\n",
    "    ext = suppl_filename.split(\".\")[-1]\n",
    "    suppl_ext.add(ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppl_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request params\n",
    "sbol_validator_url = \"https://validator.sbolstandard.org/validate/\"\n",
    "allowed_file_type = set([\n",
    "    \"gb\", \"fasta\", \"sbol\", \"txt\", \"xml\"\n",
    "])\n",
    "validator_param = {\n",
    "    'options': {\n",
    "        'language' : \"SBOL2\",\n",
    "        'test_equality': False,\n",
    "        'check_uri_compliance': False,\n",
    "        'check_completeness': False,\n",
    "        'check_best_practices': False,\n",
    "        'fail_on_first_error': True,\n",
    "        'provide_detailed_stack_trace': False,\n",
    "        'subset_uri': '',\n",
    "        'uri_prefix': 'dummy',\n",
    "        'version': '',\n",
    "        'insert_type': False\n",
    "    },\n",
    "    \"main_file\": None,\n",
    "    \"return_file\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_sequence(file):\n",
    "    \n",
    "    # restrict file size to be less than 64mb\n",
    "    file_size = os.path.getsize(file)\n",
    "    if file_size >= 64 * 2 ** 20:\n",
    "        return False, None\n",
    "    \n",
    "    # try to read the content\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        return False, None\n",
    "    \n",
    "    # validate file\n",
    "    validator_param[\"main_file\"] = content\n",
    "    res = requests.post(sbol_validator_url, json=validator_param).json()\n",
    "    return res[\"valid\"], res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the api to check if the file is a sequence file\n",
    "# we will cache the request result to reduce server load\n",
    "valid_res_cache = None\n",
    "if os.path.exists(valid_cache_path):\n",
    "    with open(valid_cache_path, \"rb\") as f:\n",
    "        valid_res_cache = pickle.load(f)\n",
    "else:\n",
    "    valid_res_cache = {}\n",
    "\n",
    "# go through all the suppl files\n",
    "for pub_num, item in tqdm(processed_files.items()):\n",
    "    for suppl_file in item[\"suppl_files\"]:\n",
    "\n",
    "        # get the name of the supplementary file\n",
    "        name = suppl_file[\"suppl_filename\"]\n",
    "\n",
    "        # only allow the following extension\n",
    "        ext = name.split(\".\")[-1]\n",
    "        if not ext in allowed_file_type:\n",
    "            continue\n",
    "        \n",
    "        # check file\n",
    "        path = os.path.join(suppl_path, suppl_file[\"rpath\"])\n",
    "        print(f\"{path}\")\n",
    "        \n",
    "        # use api or cache\n",
    "        if not suppl_file[\"rpath\"] in valid_res_cache:\n",
    "            is_valid, data = validate_sequence(path)\n",
    "            valid_res_cache[suppl_file[\"rpath\"]] = (is_valid, data)\n",
    "        else:\n",
    "            is_valid, data = valid_res_cache[suppl_file[\"rpath\"]]\n",
    "        suppl_file[\"is_sequence\"] = is_valid\n",
    "        print(f\"{name}, {is_valid}\")\n",
    "\n",
    "# save cache\n",
    "with open(valid_cache_path, \"wb\") as f:\n",
    "    pickle.dump(valid_res_cache, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files[\"sb500234s\"][\"suppl_files\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the files\n",
    "for pub_num, data in tqdm(processed_files.items()):\n",
    "    with open(os.path.join(output_path, pub_num + \".pkl\"), \"wb\") as out:\n",
    "        pickle.dump(data, out)\n",
    "    text_data = [d[\"text\"] + \"\\n\" for d in data[\"body\"]]\n",
    "    \n",
    "    # save plain text\n",
    "    if data[\"is_research\"]:\n",
    "        text_data_path = os.path.join(txt_path, \"research\")\n",
    "    else:\n",
    "        text_data_path = os.path.join(txt_path, \"non-research\")\n",
    "    with open(os.path.join(text_data_path, pub_num + \".txt\"), \"w\") as out:\n",
    "        out.writelines(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint([(n, len(d[\"suppl_files\"])) for n, d in processed_files.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check one pickle\n",
    "with open(os.path.join(output_path, \"sb500234s.pkl\"), \"rb\") as ifile:\n",
    "    pprint(pickle.load(ifile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
