{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import requests\n",
    "import pickle\n",
    "import getopt\n",
    "import subprocess as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from lxml import etree\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "input_path = os.path.abspath(\"/mnt/data1/jiawei/acs-data/article-files/\")\n",
    "output_path = os.path.abspath(\"/mnt/data1/jiawei/acs-data/processed-files/\")\n",
    "txt_path = os.path.abspath(\"/mnt/data1/jiawei/acs-data/txt-files\")\n",
    "suppl_path = os.path.abspath(\"/mnt/data1/jiawei/acs-data/suppl-files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local file parameters\n",
    "valid_cache_path = os.path.abspath(\"./sbol-validation-cache.pkl\")\n",
    "non_acs_article_path = os.path.abspath(\"./non-acs-article.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect matching files and dirs\n",
    "def collect_files(root, res, pattern=\"\", collect_dirs=True, min_depth=None, max_depth=None):\n",
    "    \n",
    "    # check max depth\n",
    "    if not max_depth is None and max_depth == 0:\n",
    "        return\n",
    "    \n",
    "    # go through all item in the dir\n",
    "    for item in os.listdir(root):\n",
    "        \n",
    "        # process item\n",
    "        item_path = os.path.join(root, item)\n",
    "        item_is_dir = os.path.isdir(item_path)\n",
    "        \n",
    "        # pull valid file in res if min depth has reached\n",
    "        if min_depth is None or min_depth - 1 <= 0:\n",
    "            if re.match(pattern, item_path):\n",
    "                if not item_is_dir or collect_dirs:\n",
    "                    res.append(item_path)\n",
    "        \n",
    "        # recursively collect all files\n",
    "        if item_is_dir:\n",
    "            next_min_depth = None if min_depth is None else min_depth - 1\n",
    "            next_max_depth = None if max_depth is None else max_depth - 1\n",
    "            collect_files(item_path, res, pattern, collect_dirs, next_min_depth, next_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helps to extract text from paragraph\n",
    "def p_helper(node):\n",
    "    \n",
    "    # <p/> does not have text\n",
    "    if node.text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # each paragarph is put into a line\n",
    "    line_list = [node.text]\n",
    "    for child in node:\n",
    "\n",
    "        # get the text inside the child if the tag isn't \n",
    "        # named-content and inline-formula\n",
    "        # and the text following the child\n",
    "        if not child.tag in (\"named-content\", \"inline-formula\"):\n",
    "            line_list.append(\" \".join(child.xpath(\".//text()\")))\n",
    "        line_list.append(child.tail)\n",
    "\n",
    "    # there might be none in line_list\n",
    "        \n",
    "    # re dark magic\n",
    "    # remove new line and spaces\n",
    "    line = \" \".join(line_list)\n",
    "    line = line.strip()\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "\n",
    "    # clean up consecutive spaces\n",
    "    line = re.sub(\"\\s+\", \" \", line)\n",
    "\n",
    "    # fix the space around punctuation\n",
    "    line = re.sub(\"\\s([.,\\):;])\", r\"\\1\", line)\n",
    "    line = re.sub(\"\\(\\s\", r\"(\", line)\n",
    "    line = re.sub(\"\\s*([-/])\\s*\", r\"\\1\", line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kwd_helper(node):\n",
    "    \n",
    "    # return a keyword string\n",
    "    kwd_tokens = node.xpath(\".//text()\")\n",
    "    kwd = \" \".join(kwd_tokens).replace(\"\\n\", \" \").strip()\n",
    "    kwd = re.sub(\"\\s+\", \" \", kwd)\n",
    "    return kwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns interesting titles\n",
    "# for example: intro, method, and results\n",
    "# return None for non interesting titles\n",
    "def title_helper(node):\n",
    "    \n",
    "    # extract text from title node\n",
    "    title = \" \".join(node.xpath(\".//text()\"))\n",
    "    title = title.replace(\"\\n\", \" \")\n",
    "    title = re.sub(\"\\s+\", \" \", title)\n",
    "    title = title.strip()\n",
    "    title = title.lower()\n",
    "    \n",
    "    # categorize title\n",
    "    res = []\n",
    "    if \"intro\" in title:\n",
    "        res.append(\"introduction\")\n",
    "    if \"result\" in title:\n",
    "        res.append(\"result\")\n",
    "    if \"discuss\" in title:\n",
    "        res.append(\"discussion\")\n",
    "    if \"material\" in title:\n",
    "        res.append(\"materials\")\n",
    "    if \"method\" in title or \"procedure\" in title:\n",
    "        res.append(\"method\")\n",
    "    if \"summary\" in title:\n",
    "        res.append(\"summary\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_body(root):\n",
    "    \n",
    "    # we are interested in the text in the body section\n",
    "    curr_title = []\n",
    "    text = []\n",
    "    text_nodes = root.xpath(\"/article/body//*[self::p or (self::title and not(ancestor::caption))]\")\n",
    "    for text_node in text_nodes:\n",
    "        \n",
    "        # handle title\n",
    "        if text_node.tag == \"title\":\n",
    "            tmp_title = title_helper(text_node)\n",
    "            if len(tmp_title) > 0:\n",
    "                curr_title = tmp_title\n",
    "            title = \" \".join(text_node.xpath(\".//text()\"))\n",
    "            title = title.replace(\"\\n\", \" \")\n",
    "            title = re.sub(\"\\s+\", \" \", title)\n",
    "            title = title.strip()\n",
    "            text.append({\n",
    "                \"text\": title,\n",
    "                \"section\": curr_title\n",
    "            })\n",
    "        \n",
    "        # handle paragraph\n",
    "        elif text_node.tag == \"p\":\n",
    "            text.append({\n",
    "                \"text\": p_helper(text_node),\n",
    "                \"section\": curr_title\n",
    "            })\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abstract(root):\n",
    "    \n",
    "    # get the abstract paragraph\n",
    "    abstract = []\n",
    "    abstract_nodes = root.xpath(\"//abstract/p\")\n",
    "    if abstract_nodes:\n",
    "        abstract.append(p_helper(abstract_nodes[0]))\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(root):\n",
    "    \n",
    "    # get the keywords\n",
    "    keywords = []\n",
    "    kwd_nodes = root.xpath(\"//kwd-group/kwd\")\n",
    "    for kwd_node in kwd_nodes:\n",
    "        keywords.append(kwd_helper(kwd_node))\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(root):\n",
    "    \n",
    "    issue_pub_date = None\n",
    "    electron_pub_date = None\n",
    "    \n",
    "    # traverse to the date note\n",
    "    date_nodes = root.xpath(\"/article/front/article-meta/pub-date\")\n",
    "    \n",
    "    # get the time\n",
    "    for node in date_nodes:\n",
    "        year = node.xpath(\"./year\")[0].text.strip()\n",
    "        month = node.xpath(\"./month\")[0].text.strip()\n",
    "        day = node.xpath(\"./day\")[0].text.strip()\n",
    "\n",
    "        if \"date-type\" in node.attrib and node.attrib[\"date-type\"] == \"issue-pub\":\n",
    "            issue_pub_date = \"%s/%s/%s\" % (month, day, year)\n",
    "        else:\n",
    "            electron_pub_date = \"%s/%s/%s\" % (month, day, year)\n",
    "    \n",
    "    return issue_pub_date, electron_pub_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id(root):\n",
    "    \n",
    "    id_node = root.xpath(\"/article/front/article-meta/article-id\")[0]\n",
    "    article_id = id_node.text.strip()\n",
    "    return article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_internal_id(root):\n",
    "    \n",
    "    id_node = root.xpath(\"/article\")[0]\n",
    "    return id_node.attrib[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_history(root):\n",
    "    \n",
    "    res = []\n",
    "    dates = root.xpath(\"/article/front/article-meta/history/date\")\n",
    "    for date in dates:\n",
    "        year = date.xpath(\"./year\")[0].text.strip()\n",
    "        month = date.xpath(\"./month\")[0].text.strip()\n",
    "        day = date.xpath(\"./day\")[0].text.strip()\n",
    "        res.append({\n",
    "            \"event\": date.attrib[\"date-type\"],\n",
    "            \"time\": \"%s/%s/%s\" % (month, day, year)\n",
    "        })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total xml files: 1597\n"
     ]
    }
   ],
   "source": [
    "# collect all xml files\n",
    "xml_paths = []\n",
    "collect_files(suppl_path, xml_paths, pattern=\".*\\.xml$\", collect_dirs=False, min_depth=1, max_depth=2)\n",
    "print(f\"total xml files: %d\" % len(xml_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't parse non acs article\n",
    "non_acs_article_list = set()\n",
    "with open(non_acs_article_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        non_acs_article_list.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c566aa3be540a883ec5df992409386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1597.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# parse the files\n",
    "processed_files = {}\n",
    "for xml_path in tqdm(xml_paths):\n",
    "\n",
    "    # print(\"\\nparsing %s\" % xml_path)\n",
    "    pub_num = xml_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    # get the root of the xml\n",
    "    root = etree.parse(xml_path).getroot()\n",
    "    \n",
    "    # get the pub date\n",
    "    issue_pub_date, electron_pub_date = extract_date(root)\n",
    "\n",
    "    # create a dictionary holding the xml data\n",
    "    xml_data = {\n",
    "        \"is_research\": not pub_num in non_acs_article_list,\n",
    "        \"keywords\": extract_keywords(root),\n",
    "        \"abstract\": extract_abstract(root),\n",
    "        \"body\": extract_body(root),\n",
    "        \"issue_pub_date\": issue_pub_date,\n",
    "        \"electron_pub_date\": electron_pub_date,\n",
    "        \"article_id\": extract_id(root),\n",
    "        \"internal_id\": extract_internal_id(root),\n",
    "        \"suppl_files\": [],\n",
    "        \"history\": extract_history(root)\n",
    "    }\n",
    "\n",
    "    # save the data\n",
    "    processed_files[pub_num] = xml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out all the zip files\n",
    "# suppl_files_zip = []\n",
    "# collect_files(suppl_path, suppl_files_zip, pattern=\".*\\.zip$\", collect_dirs=False, min_depth=3)\n",
    "# print(\"files: %d\" % len(suppl_files_zip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the zip files in place\n",
    "# for zip_file in suppl_files_zip:\n",
    "#     zip_file_dir = re.sub(\"/[^/]*$\", \"\", zip_file)\n",
    "#     res = sp.run([\"unzip\", \"-n\", zip_file, \"-d\", zip_file_dir])\n",
    "#     if res.returncode != 0:\n",
    "#         print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files: 10070\n"
     ]
    }
   ],
   "source": [
    "# collect all suppl files\n",
    "suppl_files_all = []\n",
    "collect_files(suppl_path, suppl_files_all, pattern=\"\", collect_dirs=False, min_depth=3)\n",
    "suppl_files_all = [x for x in suppl_files_all if not re.match(\".*__MACOSX.*\", x)]\n",
    "print(\"files: %d\" % len(suppl_files_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach all the suppl path to processed files\n",
    "# clear the suppl list of each article first to make the following code idempotent\n",
    "for article_info in processed_files.values():\n",
    "    article_info[\"suppl_files\"] = []\n",
    "    \n",
    "for path in suppl_files_all:\n",
    "    path = path.split(\"/\")\n",
    "    \n",
    "    # get basic attrib\n",
    "    suppl_filename = path[-1]\n",
    "    suppl_dir = suppl_path.split(\"/\")[-1]\n",
    "    suppl_dir_idx = 0\n",
    "    for i, item in enumerate(path):\n",
    "        if item == suppl_dir:\n",
    "            suppl_dir_idx = i\n",
    "            break\n",
    "    else:\n",
    "        assert False # we should not be here\n",
    "    pub_num = path[suppl_dir_idx + 1]\n",
    "    rpath = os.path.join(*path[suppl_dir_idx + 1:]) # relative path\n",
    "    \n",
    "    # create info dict\n",
    "    suppl_info = {\n",
    "        \"suppl_filename\": suppl_filename,\n",
    "        \"rpath\": rpath,\n",
    "        \"sequences\": None\n",
    "    }\n",
    "    \n",
    "    # push it into the processed files dict\n",
    "    if pub_num in processed_files:\n",
    "        processed_files[pub_num][\"suppl_files\"].append(suppl_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_suppl_file_by_ext(all_articles, allowed_ext):\n",
    "    suppl_files_to_check = []\n",
    "    for article_info in all_articles.values():\n",
    "        for suppl_info in article_info[\"suppl_files\"]:\n",
    "            # only allow the following extension\n",
    "            ext = suppl_info[\"suppl_filename\"].split(\".\")[-1]\n",
    "            if ext in allowed_ext:\n",
    "                suppl_files_to_check.append((suppl_info, article_info))\n",
    "    return suppl_files_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request params\n",
    "sbol_validator_url = \"https://validator.sbolstandard.org/validate/\"\n",
    "sbol_allowed_file_type = set([\n",
    "    \"gb\", \"fasta\", \"sbol\", \"txt\", \"xml\", \"dna\"\n",
    "])\n",
    "validator_param = {\n",
    "    'options': {\n",
    "        'language' : \"FASTA\",\n",
    "        'test_equality': False,\n",
    "        'check_uri_compliance': False,\n",
    "        'check_completeness': False,\n",
    "        'check_best_practices': False,\n",
    "        'fail_on_first_error': True,\n",
    "        'provide_detailed_stack_trace': False,\n",
    "        'subset_uri': '',\n",
    "        'uri_prefix': 'dummy',\n",
    "        'version': '',\n",
    "        'insert_type': False\n",
    "    },\n",
    "    \"main_file\": None,\n",
    "    \"return_file\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_sequence(file):\n",
    "    \n",
    "    # restrict file size to be less than 64mb\n",
    "    file_size = os.path.getsize(file)\n",
    "    if file_size >= 64 * 2 ** 20:\n",
    "        return False, None\n",
    "    \n",
    "    # try to read the content\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        return False, None\n",
    "    \n",
    "    # validate file\n",
    "    validator_param[\"main_file\"] = content\n",
    "    res = requests.post(sbol_validator_url, json=validator_param).json()\n",
    "    return res[\"valid\"], res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a243ff2cc2ca485f8dde60f121b825ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2189.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# use the api to check if the file is a sequence file\n",
    "# we will cache the request result to reduce server load\n",
    "valid_res_cache = None\n",
    "if os.path.exists(valid_cache_path):\n",
    "    with open(valid_cache_path, \"rb\") as f:\n",
    "        valid_res_cache = pickle.load(f)\n",
    "else:\n",
    "    valid_res_cache = {}\n",
    "\n",
    "# create a list of all the suppl files that we need to check\n",
    "sbol_suppl_files_to_check = filter_suppl_file_by_ext(processed_files, sbol_allowed_file_type)\n",
    "\n",
    "for suppl_info, article_info in tqdm(sbol_suppl_files_to_check):\n",
    "\n",
    "    # use api or cache to get convert and store the sequence file in fasta format\n",
    "    path = os.path.join(suppl_path, suppl_info[\"rpath\"])\n",
    "    if not suppl_info[\"rpath\"] in valid_res_cache:\n",
    "        is_valid, data = validate_sequence(path)\n",
    "        valid_res_cache[suppl_info[\"rpath\"]] = (is_valid, data)\n",
    "    else:\n",
    "        is_valid, data = valid_res_cache[suppl_info[\"rpath\"]]\n",
    "    suppl_info[\"sequences\"] = data[\"result\"] if is_valid else None\n",
    "\n",
    "# save cache\n",
    "with open(valid_cache_path, \"wb\") as f:\n",
    "    pickle.dump(valid_res_cache, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds any strings that looks like a sequence in pdf\n",
    "def extract_sequence_from_pdf(path):\n",
    "    sp.run([\"pdftotext\", \"-raw\", path, \"tmp.txt\"])\n",
    "    seqs = []\n",
    "    with open(\"tmp.txt\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        text = \"\".join(lines).upper()\n",
    "        \n",
    "        # first find all the multiline sequences\n",
    "        for multi_line_seq in re.findall(\"([ATCG]{3,}(\\n[ATCG]{3,}){1,})\", text):\n",
    "            seqs.append(re.sub(\"\\n\", \"\", multi_line_seq[0]))\n",
    "            \n",
    "        # remove extracted sequences to avoid double counting\n",
    "        text = re.sub(\"([ATCG]{3,}(\\n[ATCG]{3,}){1,})\", \" ###REMOVED### \", text)\n",
    "        \n",
    "        # then find all the one line sequences\n",
    "        for single_line_seq in re.findall(\"[ATCG]{10,}\", text):\n",
    "            seqs.append(single_line_seq)\n",
    "            \n",
    "#     sp.run([\"rm\", \"tmp.txt\"])\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b1db84e45c46d393eb4c9a83c84ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1434.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# extract sequences from pdf\n",
    "# create a list of all the suppl files that we need to check\n",
    "pdf_suppl_files_to_check = filter_suppl_file_by_ext(processed_files, [\"pdf\"])\n",
    "\n",
    "for suppl_info, article_info in tqdm(pdf_suppl_files_to_check):\n",
    "\n",
    "    # use pdftotext to extract the sequences from pdf\n",
    "    path = os.path.join(suppl_path, suppl_info[\"rpath\"])\n",
    "    sequences = extract_sequence_from_pdf(path)\n",
    "    if sequences:\n",
    "        res = []\n",
    "        for i, s in enumerate(sequences):\n",
    "            res.append(f\">{article_info['internal_id']}_{suppl_info['suppl_filename']}_{i}\\n\")\n",
    "            res.append(s + \"\\n\")\n",
    "        suppl_info[\"sequences\"] = \"\".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_sequence_from_csv(path):\n",
    "#     print(path)\n",
    "#     seqs = []\n",
    "#     with open(path, \"r\") as ifile:\n",
    "#         for line in ifile:\n",
    "#             for single_line_seq in re.findall(\"[ATCG]{10,}\", line):\n",
    "#                 seqs.append(single_line_seq)\n",
    "#     return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data1/jiawei/acs-data/suppl-files/sb9b00010/suppl/supplementary code/traindata600.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb9b00447/suppl/Supplementary_Data_4.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb5b00127/suppl/sb-2015-001275_si_008.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb5b00232/suppl/plackett_burman/level_module_assignment_2.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb5b00232/suppl/plackett_burman/level_module_assignment_3.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb5b00232/suppl/plackett_burman/level_module_assignment_1.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb5b00232/suppl/plackett_burman/plackett_burman_design_4x2.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb5b00232/suppl/plackett_burman/pathway_variant_library_1.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb5b00232/suppl/plackett_burman/pathway_variant_library_3.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb5b00232/suppl/plackett_burman/pathway_variant_library_2.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb5b00232/suppl/DNA_component_library.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb5b00232/suppl/level_module_assignments_to_full_factorials.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb7b00204/suppl/PCAP_Proteomics.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb7b00204/suppl/PCAP_GCMS.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb400201u/suppl/SI Materials/equations_dynamic_270.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb400201u/suppl/SI Materials/topo270_rank1_params.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb400201u/suppl/SI Materials/data_chart.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb400201u/suppl/SI Materials/topo270_rank1950_params.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb400201u/suppl/SI Materials/equations_static.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb400201u/suppl/SI Materials/topo270_rank1000_params.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb400201u/suppl/SI Materials/topo270_rank50_params.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb8b00398/suppl/opt-mva-master/data/rbs3/data.rbs3.ods.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb8b00398/suppl/opt-mva-master/data/rbs3.3/trainset.rbs3.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb8b00398/suppl/opt-mva-master/data/rbs3.3b/trainset.rbs3.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb8b00398/suppl/opt-mva-master/data/rbs1/trainset.rbs1.v2.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb8b00398/suppl/opt-mva-master/data/rbs1/fullset.rbs1.v2.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb8b00398/suppl/opt-mva-master/data/rbs2.2/results_RBS_MVA2_ods.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb8b00398/suppl/opt-mva-master/data/rbs2/trainset.rbs2.update.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb8b00398/suppl/opt-mva-master/data/rbs2/newfullset.v2.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb8b00398/suppl/opt-mva-master/data/rbs3.2/trainset.rbs3.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb500165g/suppl/hardware-firmware/led_board/seeed_BOM_File.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb5b00147/suppl/Prescreened sequences.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb8b00333/suppl/Supplemental Data/FileS08_T7_18h_37C.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb8b00333/suppl/Supplemental Data/FileS06_T7_18h_25C.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb8b00486/suppl/SI-supportingdata/DataS2_PKS-GFP-fusion_fitness.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb8b00486/suppl/SI-supportingdata/DataS1_LGK-GFP-fusion_fitness.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb300075t/suppl/Supplemental Files/distribute_pcr.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb300075t/suppl/Supplemental Files/Zipped_plates/oligos_plate.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb300075t/suppl/Supplemental Files/Zipped_plates/templates_plate.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb300075t/suppl/Supplemental Files/assembly.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb300075t/suppl/Supplemental Files/plate_list.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_46.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_42.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_26.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_34.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_70.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_82.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_66.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_58.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_22.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_54.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_62.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_74.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_38.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_78.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_30.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_50.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_90.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha7gw5kyn)/absorbance_86.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_46.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_42.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_26.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_34.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_70.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_82.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_66.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_58.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_22.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_54.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_62.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_74.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_38.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_78.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_30.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_50.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_90.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha6aggzvr)/absorbance_86.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_46.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_42.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_26.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_34.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_70.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_82.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_66.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_58.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_22.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_54.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_62.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_74.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_38.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_78.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_30.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_50.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_90.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r17yh8jqbjg4h)/absorbance_86.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_46.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_42.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_26.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_34.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_70.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_82.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_66.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_58.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_22.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_54.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_62.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_74.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_38.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_78.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_30.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_50.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_90.csv\n",
      "/mnt/data1/jiawei/acs-data/suppl-files/sb6b00108/suppl/SI-Data/Run of Transformation Test of Zymo10B with pUC19 Default (r185ha98fj7wg)/absorbance_86.csv\n"
     ]
    }
   ],
   "source": [
    "# xlsx_files_to_check = filter_suppl_file_by_ext(processed_files, [\"csv\"])\n",
    "# for suppl_info, article_info in xlsx_files_to_check:\n",
    "#     path = os.path.join(suppl_path, suppl_info[\"rpath\"])\n",
    "#     extract_sequence_from_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the sequence files\n",
    "for pub_num, article_info in processed_files.items():\n",
    "    seq_to_write = []\n",
    "    for suppl_info in article_info[\"suppl_files\"]:\n",
    "        if not suppl_info[\"sequences\"] is None:\n",
    "            seq_to_write.append(suppl_info[\"sequences\"])\n",
    "    if len(seq_to_write) > 0:\n",
    "        with open(os.path.join(\"sequence-files\", pub_num + \"_\" + article_info[\"internal_id\"] + \".seq.txt\"), \"w\") as outfile:\n",
    "            for seq in seq_to_write:\n",
    "                outfile.write(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the files\n",
    "for pub_num, data in tqdm(processed_files.items()):\n",
    "    with open(os.path.join(output_path, pub_num + \".pkl\"), \"wb\") as out:\n",
    "        pickle.dump(data, out)\n",
    "    text_data = [d[\"text\"] + \"\\n\" for d in data[\"body\"]]\n",
    "    \n",
    "    # save plain text\n",
    "    if data[\"is_research\"]:\n",
    "        text_data_path = os.path.join(txt_path, \"research\")\n",
    "    else:\n",
    "        text_data_path = os.path.join(txt_path, \"non-research\")\n",
    "    with open(os.path.join(text_data_path, pub_num + \".txt\"), \"w\") as out:\n",
    "        out.writelines(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check one pickle\n",
    "with open(os.path.join(output_path, \"sb9b00456.pkl\"), \"rb\") as ifile:\n",
    "    pprint(pickle.load(ifile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check history category\n",
    "event_freq = {}\n",
    "for pub_num, article_data in processed_files.items():\n",
    "    for item in article_data[\"history\"]:\n",
    "        name = item[\"event\"]\n",
    "        if not name in event_freq:\n",
    "            event_freq[name] = 0\n",
    "        event_freq[name] += 1\n",
    "pprint(event_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
