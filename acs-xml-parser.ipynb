{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import requests\n",
    "import pickle\n",
    "import subprocess as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from lxml import etree\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "# the directory that stores our supplementary data dump\n",
    "# since the supplementary data dump also contains the original article\n",
    "# we can use this as the input path\n",
    "input_path = os.path.abspath(\"/mnt/data1/jiawei/acs-data/suppl-files/\")\n",
    "\n",
    "# the directory that stores our processed pickle files\n",
    "output_path = os.path.abspath(\"/mnt/data1/jiawei/acs-data/processed-files/\")\n",
    "\n",
    "# we also provide extracted text in plain text format, this is the output path\n",
    "# of the plain text\n",
    "txt_path = os.path.abspath(\"/mnt/data1/jiawei/acs-data/txt-files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local file parameters don't change unless you know what you are doing\n",
    "\n",
    "# sbol api query cache, use to speed up the process time\n",
    "valid_cache_path = os.path.abspath(\"./sbol-validation-cache.pkl\")\n",
    "\n",
    "# deprecated, the list is inaccurate (false negative, non research article not in the list)\n",
    "non_acs_article_path = os.path.abspath(\"./non-acs-article.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbol api request params\n",
    "sbol_validator_url = \"https://validator.sbolstandard.org/validate/\"\n",
    "\n",
    "# these are the file format \n",
    "sbol_allowed_file_type = set([\n",
    "    \"gb\", \"fasta\", \"sbol\", \"txt\", \"xml\", \"dna\"\n",
    "])\n",
    "validator_param = {\n",
    "    'options': {\n",
    "        'language' : \"FASTA\",\n",
    "        'test_equality': False,\n",
    "        'check_uri_compliance': False,\n",
    "        'check_completeness': False,\n",
    "        'check_best_practices': False,\n",
    "        'fail_on_first_error': True,\n",
    "        'provide_detailed_stack_trace': False,\n",
    "        'subset_uri': '',\n",
    "        'uri_prefix': 'dummy', # we need to have something here or the api will return an error\n",
    "        'version': '',\n",
    "        'insert_type': False\n",
    "    },\n",
    "    \"main_file\": None,\n",
    "    \"return_file\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect matching files and dirs\n",
    "# basically a fancy find method\n",
    "# pattern is a re pattern\n",
    "# collect_dirs is a bool, when set to true it will collect directories/folders\n",
    "# return a list of files/directories that matches the pattern\n",
    "def collect_files(root, res, pattern=\"\", collect_dirs=True, min_depth=None, max_depth=None):\n",
    "    \n",
    "    # check max depth\n",
    "    if not max_depth is None and max_depth == 0:\n",
    "        return\n",
    "    \n",
    "    # go through all item in the dir\n",
    "    for item in os.listdir(root):\n",
    "        \n",
    "        # process item\n",
    "        item_path = os.path.join(root, item)\n",
    "        item_is_dir = os.path.isdir(item_path)\n",
    "        \n",
    "        # put valid file in res if min depth has reached\n",
    "        if min_depth is None or min_depth - 1 <= 0:\n",
    "            if re.match(pattern, item_path):\n",
    "                if not item_is_dir or collect_dirs:\n",
    "                    res.append(item_path)\n",
    "        \n",
    "        # recursively collect all files\n",
    "        if item_is_dir:\n",
    "            next_min_depth = None if min_depth is None else min_depth - 1\n",
    "            next_max_depth = None if max_depth is None else max_depth - 1\n",
    "            collect_files(item_path, res, pattern, collect_dirs, next_min_depth, next_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helps to extract text from paragraph\n",
    "def p_helper(node):\n",
    "    \n",
    "    # <p/> does not have text\n",
    "    if node.text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # each paragarph is put into a line\n",
    "    line_list = [node.text]\n",
    "    for child in node:\n",
    "\n",
    "        # get the text inside the child if the tag isn't \n",
    "        # named-content and inline-formula (those are mathametical formulas)\n",
    "        # and the text following the child\n",
    "        if not child.tag in (\"named-content\", \"inline-formula\"):\n",
    "            line_list.append(\" \".join(child.xpath(\".//text()\")))\n",
    "        line_list.append(child.tail)\n",
    "\n",
    "    # there might be none in line_list\n",
    "        \n",
    "    # re dark magic below\n",
    "    # remove new line and spaces\n",
    "    line = \" \".join(line_list)\n",
    "    line = line.strip()\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "\n",
    "    # clean up consecutive spaces\n",
    "    line = re.sub(\"\\s+\", \" \", line)\n",
    "\n",
    "    # fix the space around punctuation\n",
    "    line = re.sub(\"\\s([.,\\):;])\", r\"\\1\", line)\n",
    "    line = re.sub(\"\\(\\s\", r\"(\", line)\n",
    "    line = re.sub(\"\\s*([-/])\\s*\", r\"\\1\", line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip format from keyword nodes\n",
    "def kwd_helper(node):\n",
    "    \n",
    "    # return a keyword string\n",
    "    kwd_tokens = node.xpath(\".//text()\")\n",
    "    kwd = \" \".join(kwd_tokens).replace(\"\\n\", \" \").strip()\n",
    "    kwd = re.sub(\"\\s+\", \" \", kwd)\n",
    "    return kwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns interesting titles\n",
    "# for example: intro, method, and results\n",
    "# return None for non interesting titles\n",
    "def title_helper(node):\n",
    "    \n",
    "    # extract text from title node\n",
    "    title = \" \".join(node.xpath(\".//text()\"))\n",
    "    title = title.replace(\"\\n\", \" \")\n",
    "    title = re.sub(\"\\s+\", \" \", title)\n",
    "    title = title.strip()\n",
    "    title = title.lower()\n",
    "    \n",
    "    # categorize title\n",
    "    res = []\n",
    "    if \"intro\" in title:\n",
    "        res.append(\"introduction\")\n",
    "    if \"result\" in title:\n",
    "        res.append(\"result\")\n",
    "    if \"discuss\" in title:\n",
    "        res.append(\"discussion\")\n",
    "    if \"material\" in title:\n",
    "        res.append(\"materials\")\n",
    "    if \"method\" in title or \"procedure\" in title:\n",
    "        res.append(\"method\")\n",
    "    if \"summary\" in title:\n",
    "        res.append(\"summary\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from xpath nodes\n",
    "def extract_body(root):\n",
    "    \n",
    "    # we are interested in the text in the body section\n",
    "    curr_title = []\n",
    "    text = []\n",
    "    text_nodes = root.xpath(\"/article/body//*[self::p or (self::title and not(ancestor::caption))]\")\n",
    "    for text_node in text_nodes:\n",
    "        \n",
    "        # handle title\n",
    "        if text_node.tag == \"title\":\n",
    "            tmp_title = title_helper(text_node)\n",
    "            if len(tmp_title) > 0:\n",
    "                curr_title = tmp_title\n",
    "            title = \" \".join(text_node.xpath(\".//text()\"))\n",
    "            title = title.replace(\"\\n\", \" \")\n",
    "            title = re.sub(\"\\s+\", \" \", title)\n",
    "            title = title.strip()\n",
    "            text.append({\n",
    "                \"text\": title,\n",
    "                \"section\": curr_title\n",
    "            })\n",
    "        \n",
    "        # handle paragraph\n",
    "        elif text_node.tag == \"p\":\n",
    "            text.append({\n",
    "                \"text\": p_helper(text_node),\n",
    "                \"section\": curr_title\n",
    "            })\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract abstract\n",
    "def extract_abstract(root):\n",
    "    \n",
    "    # get the abstract paragraph\n",
    "    abstract = []\n",
    "    abstract_nodes = root.xpath(\"//abstract/p\")\n",
    "    if abstract_nodes:\n",
    "        abstract.append(p_helper(abstract_nodes[0]))\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_keywords from meta data\n",
    "def extract_keywords(root):\n",
    "    \n",
    "    # get the keywords\n",
    "    keywords = []\n",
    "    kwd_nodes = root.xpath(\"//kwd-group/kwd\")\n",
    "    for kwd_node in kwd_nodes:\n",
    "        keywords.append(kwd_helper(kwd_node))\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date information from meta data\n",
    "def extract_date(root):\n",
    "    \n",
    "    issue_pub_date = None\n",
    "    electron_pub_date = None\n",
    "    \n",
    "    # traverse to the date note\n",
    "    date_nodes = root.xpath(\"/article/front/article-meta/pub-date\")\n",
    "    \n",
    "    # get the time\n",
    "    for node in date_nodes:\n",
    "        year = node.xpath(\"./year\")[0].text.strip()\n",
    "        month = node.xpath(\"./month\")[0].text.strip()\n",
    "        day = node.xpath(\"./day\")[0].text.strip()\n",
    "\n",
    "        if \"date-type\" in node.attrib and node.attrib[\"date-type\"] == \"issue-pub\":\n",
    "            issue_pub_date = \"%s/%s/%s\" % (month, day, year)\n",
    "        else:\n",
    "            electron_pub_date = \"%s/%s/%s\" % (month, day, year)\n",
    "    \n",
    "    return issue_pub_date, electron_pub_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract article-id from meta data\n",
    "def extract_id(root):\n",
    "    \n",
    "    id_node = root.xpath(\"/article/front/article-meta/article-id\")[0]\n",
    "    article_id = id_node.text.strip()\n",
    "    return article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract internal id, the one in the first line of the xml file\n",
    "def extract_internal_id(root):\n",
    "    \n",
    "    id_node = root.xpath(\"/article\")[0]\n",
    "    return id_node.attrib[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a list of history from the meta data\n",
    "def extract_history(root):\n",
    "    \n",
    "    res = []\n",
    "    dates = root.xpath(\"/article/front/article-meta/history/date\")\n",
    "    for date in dates:\n",
    "        year = date.xpath(\"./year\")[0].text.strip()\n",
    "        month = date.xpath(\"./month\")[0].text.strip()\n",
    "        day = date.xpath(\"./day\")[0].text.strip()\n",
    "        res.append({\n",
    "            \"event\": date.attrib[\"date-type\"],\n",
    "            \"time\": \"%s/%s/%s\" % (month, day, year)\n",
    "        })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the subject of the article from meta data\n",
    "def extract_article_type(root):\n",
    "    article_type = root.xpath(\"/article/front/article-meta/article-categories/subj-group\")\n",
    "    if len(article_type) > 1:\n",
    "        print(\"article have 2 or more types\")\n",
    "    raw = article_type[0].xpath(\".//text()\")\n",
    "    # clean up\n",
    "    res = ''.join(raw).strip()\n",
    "    res = re.sub(\"\\n\", \"\", res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all xml files\n",
    "xml_paths = []\n",
    "collect_files(input_path, xml_paths, pattern=\".*\\.xml$\", collect_dirs=False, min_depth=1, max_depth=2)\n",
    "print(f\"total xml files: %d\" % len(xml_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated: get a list of non research article\n",
    "non_acs_article_list = set()\n",
    "with open(non_acs_article_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        non_acs_article_list.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the files\n",
    "# after this block, the information will be extracted to var processeed_files\n",
    "# processed_files: key(pub_num) -> value(article_info)\n",
    "processed_files = {}\n",
    "for xml_path in tqdm(xml_paths):\n",
    "\n",
    "    # print(\"\\nparsing %s\" % xml_path)\n",
    "    pub_num = xml_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    # get the root of the xml\n",
    "    root = etree.parse(xml_path).getroot()\n",
    "    \n",
    "    # get the pub date\n",
    "    issue_pub_date, electron_pub_date = extract_date(root)\n",
    "    \n",
    "    # get the article type\n",
    "    article_type = extract_article_type(root)\n",
    "\n",
    "    # create a dictionary holding the extracted data\n",
    "    xml_data = {\n",
    "        \"is_research\": \"research\" in article_type.lower(),\n",
    "        \"keywords\": extract_keywords(root),\n",
    "        \"abstract\": extract_abstract(root),\n",
    "        \"body\": extract_body(root),\n",
    "        \"issue_pub_date\": issue_pub_date,\n",
    "        \"electron_pub_date\": electron_pub_date,\n",
    "        \"article_id\": extract_id(root),\n",
    "        \"internal_id\": extract_internal_id(root),\n",
    "        \"history\": extract_history(root),\n",
    "        \"type\": article_type\n",
    "    }\n",
    "\n",
    "    # save the data\n",
    "    processed_files[pub_num] = xml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print one sample for inspection\n",
    "pprint(processed_files[\"sb6b00034\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some stats\n",
    "type_count = {}\n",
    "event_count = {}\n",
    "for k, article_info in processed_files.items():\n",
    "    article_type = article_info[\"type\"]\n",
    "    if not article_type in type_count:\n",
    "        type_count[article_type] = 0\n",
    "    type_count[article_type] += 1\n",
    "    \n",
    "    events = article_info[\"history\"]\n",
    "    for e in events:\n",
    "        if not e[\"event\"] in event_count:\n",
    "            event_count[e[\"event\"]] = 0\n",
    "        event_count[e[\"event\"]] += 1\n",
    "pprint(type_count)\n",
    "print()\n",
    "pprint(event_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out all the zip files and unzip them\n",
    "# do this once when there is new data\n",
    "# suppl_files_zip = []\n",
    "# collect_files(input_path, suppl_files_zip, pattern=\".*\\.zip$\", collect_dirs=False, min_depth=3)\n",
    "# print(\"zip files: %d\" % len(suppl_files_zip))\n",
    "# for zip_file in suppl_files_zip:\n",
    "#     zip_file_dir = re.sub(\"/[^/]*$\", \"\", zip_file)\n",
    "#     res = sp.run([\"unzip\", \"-n\", zip_file, \"-d\", zip_file_dir])\n",
    "#     if res.returncode != 0:\n",
    "#         print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all suppl files\n",
    "suppl_files_all = []\n",
    "collect_files(input_path, suppl_files_all, pattern=\"\", collect_dirs=False, min_depth=3)\n",
    "suppl_files_all = [x for x in suppl_files_all if not re.match(\".*__MACOSX.*\", x)]\n",
    "print(\"suppl files: %d\" % len(suppl_files_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach all the suppl path to processed files\n",
    "# clear the suppl list of each article first to make the following code idempotent\n",
    "for article_info in processed_files.values():\n",
    "    article_info[\"suppl_files\"] = []\n",
    "    \n",
    "for path in suppl_files_all:\n",
    "    path = path.split(\"/\")\n",
    "    \n",
    "    # get basic attrib\n",
    "    suppl_filename = path[-1]\n",
    "    suppl_dir = input_path.split(\"/\")[-1]\n",
    "    suppl_dir_idx = 0\n",
    "    for i, item in enumerate(path):\n",
    "        if item == suppl_dir:\n",
    "            suppl_dir_idx = i\n",
    "            break\n",
    "    else:\n",
    "        assert False # we should not be here\n",
    "    pub_num = path[suppl_dir_idx + 1]\n",
    "    rpath = os.path.join(*path[suppl_dir_idx + 1:]) # relative path\n",
    "    \n",
    "    # create info dict\n",
    "    suppl_info = {\n",
    "        \"suppl_filename\": suppl_filename,\n",
    "        \"rpath\": rpath,\n",
    "        \"sequences\": None\n",
    "    }\n",
    "    \n",
    "    # push it into the processed files dict\n",
    "    if pub_num in processed_files:\n",
    "        processed_files[pub_num][\"suppl_files\"].append(suppl_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get supplementary files by extension\n",
    "def filter_suppl_file_by_ext(all_articles, allowed_ext):\n",
    "    suppl_files_to_check = []\n",
    "    for article_info in all_articles.values():\n",
    "        for suppl_info in article_info[\"suppl_files\"]:\n",
    "            # only allow the following extension\n",
    "            ext = suppl_info[\"suppl_filename\"].split(\".\")[-1]\n",
    "            if ext in allowed_ext:\n",
    "                suppl_files_to_check.append((suppl_info, article_info))\n",
    "    return suppl_files_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sbol api to validate sequence files\n",
    "def validate_sequence(file):\n",
    "    \n",
    "    # restrict file size to be less than 64mb\n",
    "    # this is an api restriction\n",
    "    file_size = os.path.getsize(file)\n",
    "    if file_size >= 64 * 2 ** 20:\n",
    "        return False, None\n",
    "    \n",
    "    # try to read the content\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        return False, None\n",
    "    \n",
    "    # validate file\n",
    "    validator_param[\"main_file\"] = content\n",
    "    res = requests.post(sbol_validator_url, json=validator_param).json()\n",
    "    return res[\"valid\"], res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the api to check if the file is a sequence file\n",
    "# we will cache the request result to reduce server load\n",
    "valid_res_cache = None\n",
    "if os.path.exists(valid_cache_path):\n",
    "    with open(valid_cache_path, \"rb\") as f:\n",
    "        valid_res_cache = pickle.load(f)\n",
    "else:\n",
    "    valid_res_cache = {}\n",
    "\n",
    "# create a list of all the suppl files that we need to check\n",
    "sbol_suppl_files_to_check = filter_suppl_file_by_ext(processed_files, sbol_allowed_file_type)\n",
    "\n",
    "for suppl_info, article_info in tqdm(sbol_suppl_files_to_check):\n",
    "\n",
    "    # use api or cache to get convert and store the sequence file in fasta format\n",
    "    path = os.path.join(input_path, suppl_info[\"rpath\"])\n",
    "    if not suppl_info[\"rpath\"] in valid_res_cache:\n",
    "        is_valid, data = validate_sequence(path)\n",
    "        valid_res_cache[suppl_info[\"rpath\"]] = (is_valid, data)\n",
    "    else:\n",
    "        is_valid, data = valid_res_cache[suppl_info[\"rpath\"]]\n",
    "    suppl_info[\"sequences\"] = data[\"result\"] if is_valid else None\n",
    "\n",
    "# save cache\n",
    "with open(valid_cache_path, \"wb\") as f:\n",
    "    pickle.dump(valid_res_cache, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds any strings that looks like a sequence in pdf\n",
    "# need to have pdftotext installed\n",
    "def extract_sequence_from_pdf(path):\n",
    "    sp.run([\"pdftotext\", \"-raw\", path, \"/tmp/sbks-pdf-tmp.txt\"]) # use tmp dir maybe it is ram so faster?\n",
    "    seqs = []\n",
    "    with open(\"/tmp/sbks-pdf-tmp.txt\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        text = \"\".join(lines).upper()\n",
    "        \n",
    "        # first find all the multiline sequences\n",
    "        for multi_line_seq in re.findall(\"([ATCG]{3,}(\\n[ATCG]{3,}){1,})\", text):\n",
    "            seqs.append(re.sub(\"\\n\", \"\", multi_line_seq[0]))\n",
    "            \n",
    "        # remove extracted sequences to avoid double counting\n",
    "        text = re.sub(\"([ATCG]{3,}(\\n[ATCG]{3,}){1,})\", \" ###REMOVED### \", text)\n",
    "        \n",
    "        # then find all the one line sequences\n",
    "        for single_line_seq in re.findall(\"[ATCG]{10,}\", text):\n",
    "            seqs.append(single_line_seq)\n",
    "            \n",
    "#     sp.run([\"rm\", \"tmp.txt\"])\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sequences from pdf\n",
    "# create a list of all the suppl files that we need to check\n",
    "pdf_suppl_files_to_check = filter_suppl_file_by_ext(processed_files, set([\"pdf\"]))\n",
    "\n",
    "for suppl_info, article_info in tqdm(pdf_suppl_files_to_check):\n",
    "\n",
    "    # use pdftotext to extract the sequences from pdf\n",
    "    path = os.path.join(input_path, suppl_info[\"rpath\"])\n",
    "    sequences = extract_sequence_from_pdf(path)\n",
    "    if sequences:\n",
    "        res = []\n",
    "        for i, s in enumerate(sequences):\n",
    "            res.append(f\">{article_info['internal_id']}_{suppl_info['suppl_filename']}_{i}\\n\")\n",
    "            res.append(s + \"\\n\")\n",
    "        suppl_info[\"sequences\"] = \"\".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_sequence_from_csv(path):\n",
    "#     print(path)\n",
    "#     seqs = []\n",
    "#     with open(path, \"r\") as ifile:\n",
    "#         for line in ifile:\n",
    "#             for single_line_seq in re.findall(\"[ATCG]{10,}\", line):\n",
    "#                 seqs.append(single_line_seq)\n",
    "#     return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlsx_files_to_check = filter_suppl_file_by_ext(processed_files, [\"csv\"])\n",
    "# for suppl_info, article_info in xlsx_files_to_check:\n",
    "#     path = os.path.join(suppl_path, suppl_info[\"rpath\"])\n",
    "#     extract_sequence_from_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the sequence files\n",
    "for pub_num, article_info in processed_files.items():\n",
    "    seq_to_write = []\n",
    "    for suppl_info in article_info[\"suppl_files\"]:\n",
    "        if not suppl_info[\"sequences\"] is None:\n",
    "            seq_to_write.append(suppl_info[\"sequences\"])\n",
    "    if len(seq_to_write) > 0:\n",
    "        with open(os.path.join(\"sequence-files\", pub_num + \"_\" + article_info[\"internal_id\"] + \".seq.txt\"), \"w\") as outfile:\n",
    "            for seq in seq_to_write:\n",
    "                outfile.write(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the processed files\n",
    "for pub_num, data in tqdm(processed_files.items()):\n",
    "    with open(os.path.join(output_path, pub_num + \".pkl\"), \"wb\") as out:\n",
    "        pickle.dump(data, out)\n",
    "    text_data = [d[\"text\"] + \"\\n\" for d in data[\"body\"]]\n",
    "    \n",
    "    # save plain text\n",
    "    if data[\"is_research\"]:\n",
    "        text_data_path = os.path.join(txt_path, \"research\")\n",
    "    else:\n",
    "        text_data_path = os.path.join(txt_path, \"non-research\")\n",
    "    with open(os.path.join(text_data_path, pub_num + \".txt\"), \"w\") as out:\n",
    "        out.writelines(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check one pickle\n",
    "with open(os.path.join(output_path, \"sb300092n.pkl\"), \"rb\") as ifile:\n",
    "    pprint(pickle.load(ifile)[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
